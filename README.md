# DRL_P2_Continuous-Control

## Project Details

This project is based on the section of the following github repository.

https://github.com/udacity/deep-reinforcement-learning

### Environment


![Reacher](https://video.udacity-data.com/topher/2018/June/5b1ea778_reacher/reacher.gif)


Please refer to [Reacher](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Learning-Environment-Examples.md#reacher) for details.

### Reward


### State space



### Action spaces


### Training end condition - when the environment is considered solved

The task is episodic, and in order to solve the environment, the agent must get an average score of +13 over 100 consecutive episodes.


## Getting Started

### Library Requirement

### GPU Library
https://pytorch.org/?utm_source=Google&utm_medium=PaidSearch&utm_campaign=%2A%2ALP+-+TM+-+General+-+HV+-+JP&utm_adgroup=PyTorch+Version&utm_keyword=%2Bpytorch%20%2Bversion&utm_offering=AI&utm_Product=PyTorch&gclid=Cj0KCQjw84XtBRDWARIsAAU1aM2zq_aOaEVOMcbRXR5UUKbRyy6k2amUoQ7J8z88762R1Y5pxokc_RsaArgrEALw_wcB

``
conda install pytorch torchvision cudatoolkit=10.1 -c pytorch
``

### Unity Environment

#### Version 1: One (1) Agent
- Linux: click [here]()
- Mac OSX: click [here]()
- Windows (32-bit): [here]()
- Windows (64-bit): [here]()
#### Version 2: Twenty (20) Agents
- Linux: click [here]()
- Mac OSX: click [here]()
- Windows (32-bit): [here]()
- Windows (64-bit): [here]()

## Instructions



### Option 1

Refer to [Continuous_Control_single.ipynb](./Continuous_Control_single.ipynb)

### Option 2

Refer to [Continuous_Control_multi.ipynb](./Continuous_Control_multi.ipynb)

